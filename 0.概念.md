
### 状态价值（State Value） 
- **定义**：状态价值是指在给定状态下，智能体遵循某种策略**直到结束**所能获得的长期累积奖励的期望。它表示了智能体处于某个特定状态时，从该状态开始往后所能获得的总回报的预估。 
- **数学表示**：通常用$V^{\pi}(s)$表示在策略 $\pi$下状态 $s$ 的价值。例如，在一个机器人导航任务中，假设机器人处于房间的某个位置 $s$，如果它按照当前的行为策略$\pi$ 行动，可能会在未来的一系列步骤中获得一些奖励（比如成功到达目标位置得到正奖励，碰到障碍物得到负奖励等），那么 $V^{\pi}{s}$ 就是对这些未来奖励的一个预期总和。 
- **作用**：帮助智能体评估不同状态的好坏，以便在决策时选择更有利于长期累积奖励的状态。智能体可以通过比较不同状态的价值来决定下一步的行动方向，倾向于朝着价值更高的状态移动。 

### 动作价值（Action Value） 
- **定义**：动作价值是指在给定状态下，执行某个特定动作后，遵循某种策略**直到结束**(并不是到最终目标，也可以半途终止）所能获得的长期累积奖励的期望。它不仅考虑了当前状态，还考虑了在该状态下执行特定动作所带来的影响。 
- **数学表示**：用 $Q^{\pi}(s, a)$表示在策略$\pi$ 下，状态 $s$执行动作 $a$的价值。例如，在玩扑克牌游戏时，当前你手中的牌型是状态 $s$，你可以选择的动作有出牌、停牌等，假设你选择了出牌这个动作 $a$，那么$Q^{\pi}(s, a)$就是在你当前牌型状态下选择出牌这个动作后，根据你玩牌的策略$\pi$，未来可能获得的总奖励的预期。 
- **作用**：用于评估在特定状态下每个可选动作的优劣，指导智能体在当前状态下选择最优的动作。智能体通过比较当前状态下不同动作的动作价值，选择具有最高动作价值的动作来执行，以期望获得最大的长期累积奖励。 
- 
### 两者关系 

- 状态价值 $V^{\pi}(s)$ 可以通过对该状态下所有可能动作的动作价值 $Q^{\pi}(s, a)$进行加权平均得到，权重是在策略$\pi$下选择每个动作的概率。即 $$V^{\pi}(s)=\sum_{a\in A}\pi(a|s)Q^{\pi}(s,a))$$，其中 $A$是动作空间。 
- 动作价值 $Q^{\pi}(s, a)$\) $则与状态价值$ $V^{\pi}(s)$ 相互关联，它考虑了从当前状态执行特定动作后，转移到下一个状态的价值变化以及即时奖励。在计算动作价值时，需要考虑到执行动作后可能到达的各种新状态及其对应的状态价值。 总的来说，状态价值帮助智能体从宏观上评估状态的好坏，而动作价值则更侧重于指导智能体在具体状态下的动作选择，两者共同作用，帮助智能体在强化学习环境中学习到最优的行为策略，以最大化长期累积奖励。


### 智能体 (agent)
由谁做动作或决策,谁就是智能体。

### 马尔可夫决策过程(Markov decision process,MDP)
一个 MDP 通常由状态空间、动作空间、状态转移函数、奖励函数、折扣因子等组成。

### 状态（State）
在每个时刻,环境有一个状态 (state),可以理解为对当前时刻环境的概括。

### 状态空间(state space)
指所有可能存在状态的集合,记作花体字母$\mathcal{S}$。状态空间 可以是离散的,也可以是连续的。状态空间可以是有限集合,也可以是无限可数集合。

### 动作（action)
动作（ action ） 是智能体基于当前状态所做出的决策。


### 动作空间（action space)
动作空间(action space)是指所有可能动作的集合,记作花体字母 $\mathcal{A}$。

### 奖励(reward)
 奖励 （ reward ） 是指在智能体执行一个动作之后， 环境返回给智能体的一个数值。
 - 通常假设奖励是当前状态 s、当前动作 a、下一时刻状态 s′ 的函数,把奖励函数记作  r(s, a, s′)。
-  有时假设奖励仅仅是 s 和 a 的函数,记作 r(s, a)。
- 我们总是假设奖励函数是 有界的,即对于所有 $a\in\mathcal{A}$ 和 s, $s′ \in \mathcal{S}$ ,有 $\lvert r(s, a, s′)\rvert|\ < \infty$。
### 状态转移 （State Transaction）
状态转移(state transition) 是指智能体从当前 t 时刻的状态 s 转移到下一个时刻状态为 s′ 的过程。

### 状态转移概率函数 （state transition probability function）
 
 用于描述状态转移，记作$$ p_{t}(s′\mid s, a ) = \mathbb{P}(S′_{t +1}  = s′ \mid S_{t} =  s, A_{t} = a)$$, 表示这个事件的概率：在当前状态  s ，智能体执行动作  a ，环境的状态变成  s′。
- 状态转移可以是**确定性的**。给定当前的状态 s ，智能体执行动作 a ，环境用某个函数 $\tau_{t}$  计算出新的状态 $s′ = \tau t(s,a)$
- 确定状态转移是随机状态转移的一个特例，通常假设状态转移概率函数是平稳的（stationary）$$p_{t}(s' \mid s ,a)=
\begin{cases}
1,\quad \tau_{t}(s,a)=s'\\
0,\quad otherwise
\end{cases}
$$
### 策略 (policy)
根据观测到的状态,如何做出决策,即如何从动作空间中选  取一个动作。**强化学习的目标就是得到一个策略函数,在每个时刻根据观测到的状态做出决策。**

- **随机策略** 把状态记作 S 或 s,动作记作 A 或 a,随机策略函数$\pi$ : $\mathcal{S} ×\mathcal{A} \mapsto [0, 1]$  是一个概率密度函数:  π(a|s) = P(A = a ∣∣ S = s). $$\pi(a \mid s) = \mathbb{P}(A=a \mid S=s)$$ 策略函数的输入是状态 s 和动作 a,输出是一个 0 到 1 之间的概率值。	
- **确定策略** 把状态记作 S 或 s,动作记作 A 或 a,确定策略函数$\pi$ : $\mathcal{S} \mapsto \mathcal{A}$。它把状态 s 作为输入,直接输出动作 $a = \mu (s)$,  而不是输出概率值。对于给定的状态 s,做出的决策 a 是确定的,没有随机性。

### 智能体与环境交互(agent environment interaction) 
指智能体观测到环境的状态  s,做出动作 a,动作会改变环境的状态,环境反馈给智能体奖励 r 以及新的状态 s′。
![[Pasted image 20250301094653.png]]