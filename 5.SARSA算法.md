Q-Learning的目的是学习到**最优的动作价值函数$Q_\star$**
SARSA的目的是学习**动作价值函数$Q_\pi(s,a)$**
SARSA是$State-Action-Reward-State-Action$的简称

$Q_\pi$被用来评价策略的优劣，而非用于控制Agent
$Q_\pi$与策略函数$\pi$结合使用的模型本称作$actor-critic$方法
#### actor
是策略函数$\pi$用于控制智能体，看作是“演员”,可以理解为一个决策者，它根据当前环境的状态来选择要执行的动作。Actor 会不断调整自己的策略，使得在长期内获得的奖励最大化。例如，在一个机器人行走的任务中，Actor 就是决定机器人下一步是向前走、向左转还是向右转等动作的 “大脑”。
#### critic
$Q_\pi$用来评价$\pi$的表现，帮助改进$\pi$，看作是“评委”.它会评估 Actor 的表现，也就是评估当前状态下采取某个动作的好坏程度。Critic 通过学习一个价值函数来实现这个评估，这个价值函数会告诉我们在某个状态下，采取一系列动作后能够获得的长期奖励的估计值。例如，对于机器人行走的任务，Critic 会根据机器人当前的位置、姿态等状态，以及它采取的动作，来判断这个动作是否有助于机器人更快地到达目标位置，如果是，就给予较高的评价，反之则给予较低的评价。

$actor-critic$方法通常使用SARSA训练“评委”$Q_\pi$

### SARSA学习算法

$$Q_\pi(s_t,a_t)=\mathbb{E}_{S_{t+1},A_{t+1}}\left[R_t+\gamma\cdot Q_\pi(S_{t+1},A_{t+1})\mid S_t=t,A_t=a_t\right]$$
方程左边可以近似成$q(s_t,a_t)$,这个值是在Q表中在t时刻对$Q_\pi(s_t,a_t)$做出的估计
方程右边的期望是关于下一个时刻状态$S_{t+1}$和动作$A_{t+1}$求的。给定当前状态$s_t$,Agent采取动作$a_t$,环境给出反馈奖励$r_t$和新状态$s_{t+1}$.然后基于$s_{t+1}$做随机抽样，得到一个新的动作$$\widetilde{a}_{t+1} \approx \pi(\cdot \mid s_{t+1}) $$
用观测到的$r_t$ 、$s_{t+1}$和计算出的$\widetilde{a}_{t+1}$对期望做蒙特卡洛近似，得到：$$r_t+\gamma \cdot Q_\pi(s_{t+1},\widetilde{a}_{t+1})$$
把$Q_\pi$近似成$q$，得到$$\hat y_t \triangleq r_t+\gamma \cdot q(s_{t+1},\widetilde{a}_{t+1})$$
这个值就是TD目标。它是Q表中在$t+1$时刻对$Q_\pi(s_t,a_t)$做出的估计。这个值是基于真实观测到的奖励$r_t$得来的，所以要用这个值来更新Q表上的元素：$$q(s_{t},a_{t})\leftarrow(1-\alpha)\cdot q(s_t,a_t)+\alpha\cdot \hat{y}_t$$

## 训练流程
整体设定：假设我们有一个智能体在一个环境中进行学习，智能体通过与环境交互来改进自己的行为策略。
- 一个是 Q 表（用$q_{now}$和$q_{new}$表示不同时刻的 Q 表），用于记录在不同状态下采取不同动作的价值；
- 另一个是当前策略$\pi_{now}$，决定智能体在某个状态下选择动作的方式 。

1. **当前状态下选择动作**：智能体先观察到当前状态$s_t$，然后根据现有的策略$\pi_{now}$，从这个状态下所有可能的动作中抽样选择一个动作$a_t$。：$a_t\sim\pi_{now}(\cdot \mid s_t)$
2. **记录当前动作的 Q 值:** 更新表格中位于$(s_t,a_t)$位置上的元素$$\hat q_t=q_{now}(s_t,a_t)$$
3. **执行动作并获取反馈:** Agent执行动作$a_t$之后，获得奖励$r_t$和转移到新的状态$s_{t+1}$
4. **设想下一个动作:** 根据当前策略做抽样:$\widetilde{a}_{t+1} \approx \pi_{now}(\cdot \mid s_{t+1})$，这里**只是设想**而不是去做，主要是为了采用一种更具 **“前瞻性”** 的学习方式 ，在 SARSA 算法中，是根据当前策略选择一个动作执行后，基于执行后的下一个实际采取的动作来更新 Q 表。而这里这种方式在更新 Q 表时，不依赖于实际执行的下一个动作，通过**设想下一个状态的动作**来计算目标值，能让智能体在学习过程中朝着最优策略的方向去估计 Q 值。这样可以让智能体在学习过程中，综合考虑未来可能的 “最优” 情况，而不局限于当前策略下实际执行的动作，从而更有效地探索环境和学习到最优策略。
5. **记录设想动作的 Q 值：** 把表格$q_{now}$中位于$(s_{t+1},\widetilde a_{t+1})$位置上的元素记作：$$\hat q_{t+1}=q_{now}(s_t,\widetilde a_{t+1})$$
6. **计算TD目标和TD误差：**
- TD 目标$\widehat{y}_t$的计算是结合刚刚得到的奖励$r_t$和设想动作的 Q 值$\widehat{q}_{t+1}$ ，再乘上一个折扣因子$\gamma$（用来体现未来奖励的重要程度，一般是 0 到 1 之间的数，越接近 1 说明越看重未来奖励）。这一步是在预估从当前状态执行动作后，未来能获得的总价值。
- TD 误差$\delta_t$则是用之前记录的当前动作 Q 值$\widehat{q}_t$减去计算出的 TD 目标$\widehat{y}_t$，它反映了当前估计的 Q 值和实际预估的总价值之间的差距。$$\hat y_t=r_t+\gamma\cdot \hat q_{t+1},\delta_t=\hat q_t-\hat y_t$$
1. **更新Q表：** 更新表格中（$s_t,a_t$）位置上的元素：$$q_{new}(s_t,a_t)\leftarrow q_{now}(s_t,a_t)-\alpha \cdot \delta_t$$
2. **更新策略函数：** 使用某种算法更新策略函数


## Q Learning和Sarsa的区别

Sarsa算法是一种TD（时序差分）算法，是TD算法在策略评估和控制方面的具体应用，因此不存在Sarsa算法和TD算法的严格区分，不过可以说Sarsa算法与其他一些TD算法（如Q - learning）存在区别，具体如下： 
- **更新策略不同** 
	- **Sarsa**：是一种**On Policy**的方法，其更新公式为$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t + 1}+\gamma Q(S_{t + 1}, A_{t + 1}) - Q(S_t, A_t))$$它在更新值函数时，考虑的是在新状态下**实际采取**的下一个动作的值，即更新策略是“**状态 - 动作 - 奖励 - 下一个状态 - 下一个动作**”。
	- **Q - learning**：是一种**Off Policy**方法，更新公式为$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t + 1}+\gamma \max_a Q(S_{t + 1}, a) - Q(S_t, A_t))$$它在更新时选择的是新状态下**使值函数最大化的动作**，而不考虑实际采取的动作，更新策略是 **“状态 - 动作 - 奖励 - 最大值动作”。** 
- **学习方式不同**
	- **Sarsa**：更倾向于**跟随当前策略**进行学习，因此学习过程相对稳定。**不能使用经验回放。** 由于它考虑了实际执行的下一个动作，所以对于策略的各种特征能够较好地学习到，但收敛速度可能较慢。例如在一些需要逐步探索环境、积累经验的任务中，Sarsa能够稳步地根据当前策略来更新价值函数，逐渐优化策略。 
	- **Q  learning**：更倾向于**学习最优策略**，它总是朝着最大化价值函数的方向更新。**可以使用经验回放。** 不过，这也可能导致学习过程不稳定，容易受到噪声干扰，因为它选择的最大Q值动作可能在实际中并不是最优的长期选择。比如在复杂环境中，Q  learning可能会因为过度追求当前的最大Q值而陷入局部最优，难以找到全局最优策略。 
- **探索策略不同** 
	- **Sarsa**：在学习过程中倾向于跟随当前策略进行探索，探索方式相对较为**保守**。它会根据当前策略在新状态下选择动作，然后根据这个动作的结果来更新价值函数，所以在探索新的状态 - 动作对时比较**谨慎**，更**适合于需要较多探索的任务**。
	- **Q  learning**：由于其基于最大Q值的更新方式，可能会导致在某些情况下**过度探索**。因为它总是试图选择具有最大Q值的动作，而**忽略了实际执行动作的结果**，这可能使得它在一些不必要的状态 - 动作对上进行过多的探索，从而**陷入不收敛**的状态。 
- **应用场景不同** 
	- **Sarsa**：适用于需要**稳定学习过程**、**重视探索**的任务，或者在**与环境进行交互时进行在线学习**的情况。例如，在一些实时性要求较高、环境变化较为复杂的场景中，如自动驾驶、机器人实时控制等，Sarsa能够根据当前的实际情况进行稳定的学习和决策。 
	- **Q  learning**：适用于那些能够**明确定义最优策略**，并且**希望快速找到最优解**的题。例如，在一些游戏场景中，目标是找到能够获得最高分数的最优策略，Q  learning可以通过不断地尝试不同的动作来快速逼近最优解。

## 神经网络形式的SARSA

**价值网络**
如果状态空间$\mathcal{S}$是无限集，则无法用一张有限的表格表示$Q_{\pi}$，但是可以使用一个神经网络$q(s,a;\boldsymbol{w})$来近似。这个网络被称为价值网络（value network）。
该网络的输入是状态s，输出是动作a的价值。
**算法推导**
给定当前状态 $s_t$，智能体执行动作 $a_t$，环境会给出奖励 $r_t$ 和新的状态 $s_{t + 1}$。
然后基于 $s_{t + 1}$ 做随机抽样，得到新的动作 $\tilde{a}_{t + 1} \sim \pi(\cdot|s_{t + 1})$。
定义 TD 目标： $$ \widehat{y}_t \triangleq r_t + \gamma \cdot q(s_{t + 1},\tilde{a}_{t + 1};\boldsymbol{w}). $$ 目的是让 $q(s_t,a_t;\boldsymbol{w})$ 接近 TD 目标 $\widehat{y}_t$，所以定义损失函数： $$ L(\boldsymbol{w}) \triangleq \frac{1}{2}[q(s_t,a_t;\boldsymbol{w}) - \widehat{y}_t]^2. $$ 损失函数的变量是 $\boldsymbol{w}$，而 $\widehat{y}_t$ 被视为常数。（尽管 $\widehat{y}_t$ 也依赖于参数 $\boldsymbol{w}$，但这一点被忽略 掉。）
设 $\widehat{q}_t = q(s_t,a_t;\boldsymbol{w})$。损失函数关于 $\boldsymbol{w}$ 的梯度是： $$\nabla_{\boldsymbol{w}} L(\boldsymbol{w}) = \underbrace{(\widehat{q}_t - \widehat{y}_t)}_{\text{TD 误差 }\delta_t} \cdot \nabla_{\boldsymbol{w}} q(s_t,a_t;\boldsymbol{w}).$$
做一次梯度下降，更新$\boldsymbol{w}$$ $$\boldsymbol{w}\leftarrow$\boldsymbol{w}$-\alpha\cdot\delta_t\cdot\nabla_{\boldsymbol{w}}q(s_t,a_t;\boldsymbol{w})$$

**训练流程**
设当前价值网络的参数为$\boldsymbol{w}_{now}$，当前策略为$\pi_{now}$。
每一轮训练用五元组$(s_t, a_t, r_t, s_{t + 1}, \tilde{a}_{t + 1})$对价值网络参数做一次更新。
1. 观测到当前状态$s_t$，根据当前策略做抽样：$a_t \sim \pi_{now}(\cdot|s_t)$。
2. 用价值网络计算$(s_t, a_t)$的价值： $\widehat{q}_t = q(s_t, a_t; \boldsymbol{w}_{now}).$ 
3. 智能体执行动作$a_t$之后，观测到奖励$r_t$和新的状态$s_{t + 1}$。 
4. 根据当前策略做抽样：$\tilde{a}_{t + 1} \sim \pi_{now}(\cdot|s_{t + 1})$。注意，$\tilde{a}_{t + 1}$只是假想的动作，智能体不予执行。
5. 用价值网络计算$(s_{t + 1}, \tilde{a}_{t + 1})$的价值： $\widehat{q}_{t + 1} = q(s_{t + 1}, \tilde{a}_{t + 1}; \boldsymbol{w}_{now}).$ 
6. 计算TD目标和TD误差： $\widehat{y}_t = r_t + \gamma \cdot \widehat{q}_{t + 1}, \quad \delta_t = \widehat{q}_t - \widehat{y}_t.$ 
7. 对价值网络$q$做反向传播，计算$q$关于$\boldsymbol{w}$的梯度：$\nabla_{\boldsymbol{w}}q(s_t, a_t; \boldsymbol{w}_{now})$。
8. 更新价值网络参数： $\boldsymbol{w}_{new} \leftarrow \boldsymbol{w}_{now} - \alpha \cdot \delta_t \cdot \nabla_{\boldsymbol{w}}q(s_t, a_t; \boldsymbol{w}_{now}).$ 
9. 用某种算法更新策略函数。该算法与SARSA算法无关。

### Gym环境下基于神经网络控制CartPole例程
```python
import gym

import torch

import torch.nn as nn

import torch.optim as optim

import numpy as np

import random

import pygame

import matplotlib.pyplot as plt

  

# 定义 Q 网络

class QNetwork(nn.Module):

    def __init__(self, state_size, action_size):

        super(QNetwork, self).__init__()

        self.fc1 = nn.Linear(state_size, 128)

        self.fc2 = nn.Linear(128, 128)

        self.fc3 = nn.Linear(128, action_size)

    def forward(self, x):

        x = torch.relu(self.fc1(x))

        x = torch.relu(self.fc2(x))

        return self.fc3(x)

  

# 定义 SARSA 代理

class SARSAgent:

    def __init__(self, state_size, action_size):

        self.state_size = state_size

        self.action_size = action_size

        self.gamma = 0.99

        self.epsilon = 1.0

        self.epsilon_min = 0.01

        self.epsilon_decay = 0.995

        self.learning_rate = 0.001

        self.model = QNetwork(state_size, action_size)

        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)

        self.criterion = nn.MSELoss()

    def act(self, state):

        if np.random.rand() <= self.epsilon:

            return random.randrange(self.action_size)

        state = np.array(state)

        if len(state.shape) != 2 or state.shape[0] != 1:

            raise ValueError("State should be a 2D array with shape (1, state_size).")

        state = torch.FloatTensor(state)

        act_values = self.model(state)

        action = np.argmax(act_values.detach().numpy())

        return action

    def update(self, state, action, reward, next_state, next_action, done):

        # 将状态和下一个状态转换为numpy数组并指定数据类型

        state = np.array(state, dtype=np.float32)

        next_state = np.array(next_state, dtype=np.float32)

        # 将numpy数组转换为PyTorch张量

        state = torch.FloatTensor(state)

        next_state = torch.FloatTensor(next_state)

        # 获取当前状态-动作对的Q值

        current_q = self.model(state)[0][action]

        # 获取下一个状态-动作对的Q值，如果回合结束则为0

        next_q = self.model(next_state)[0][next_action] if not done else torch.tensor(0.0, dtype=torch.float32)

        # 计算目标Q值：即时奖励 + 折扣因子 * 下一个Q值

        target = torch.tensor(reward, dtype=torch.float32) + self.gamma * next_q

        # 计算当前Q值和目标Q值之间的均方误差损失

        loss = self.criterion(current_q, target.detach())

        # 梯度清零

        self.optimizer.zero_grad()

        # 反向传播计算梯度

        loss.backward()

        # 更新网络参数

        self.optimizer.step()

        # 衰减探索率，但保持不低于最小值

        if self.epsilon > self.epsilon_min:

            self.epsilon *= self.epsilon_decay

  

if __name__ == "__main__":

    EPISODES = 500

    env = gym.make("CartPole-v1", render_mode="rgb_array")

    pygame.init()

    screen = pygame.display.set_mode((800, 600))

    font = pygame.font.SysFont("arial", 20)

    state_size = env.observation_space.shape[0]

    action_size = env.action_space.n

    agent = SARSAgent(state_size, action_size)

    scores_list = []

    for e in range(EPISODES):

        state, _ = env.reset()

        state = np.reshape(state, [1, state_size])

        action = agent.act(state)

        for time in range(1000):

            for event in pygame.event.get():

                if event.type == pygame.QUIT:

                    env.close()

                    pygame.quit()

                    quit()

            frame = env.render()

            if frame is not None:

                surface = pygame.surfarray.make_surface(np.transpose(frame, (1, 0, 2)))

                text_surface = font.render(

                    f"Episode: {e+1} Score: {time}", True, (255, 0, 0)

                )

                scaled_surface = pygame.transform.scale(surface, (800, 600))

                scaled_surface.blit(text_surface, (10, 10))

                screen.blit(scaled_surface, (0, 0))

                pygame.display.flip()

            next_state, reward, terminated, truncated, _ = env.step(action)

            done = terminated or truncated

            reward = reward if not done else -10

            next_state = np.array(next_state, dtype=np.float32).flatten()

            next_state = np.reshape(next_state, [1, state_size])

            next_action = agent.act(next_state)

            agent.update(state, action, reward, next_state, next_action, done)

            state = next_state

            action = next_action

            if done:

                print(

                    f"Episode: {e + 1}/{EPISODES}, Score: {time}, Epsilon: {agent.epsilon:.2f}"

                )

                break

            scores_list.append(time)

        if e % 100 == 0:

            torch.save(agent.model.state_dict(), f"sarsa_model_{e}.pth")

    plt.plot(scores_list)

    plt.xlabel('Episode')

    plt.ylabel('Score')

    plt.title('Training Scores')

    plt.show()

    env.close()

    pygame.quit()

```


![[sarsa1.gif]]
![[Figure_1.png]]