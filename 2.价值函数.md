
# 2.1 动作价值函数 Action Value Function

因为$U_{t}$的随机性让我们无法判断局势优劣，所以就要对$U_{t}$求期望，用于消除其随机性。

**$\star$期望为什么能够消除随机性**
期望之所以能在一定程度上消除随机性，是因为它通过对随机变量所有可能取值进行加权平均，得到一个代表中心趋势的数值，具体如下： 
- **综合考虑所有可能结果**：期望的计算考虑了随机变量的所有可能取值及其对应的概率。以掷骰子为例，虽然每次掷骰子的结果是随机的，可能是1到6中的任何一个数，但通过计算期望，我们得到了一个综合反映所有可能结果的平均数值。这个数值不是某次具体掷骰子的结果，而是在大量重复试验下，所有结果的平均表现，从而在一定程度上消除了单次试验结果的随机性。
- **体现长期平均行为**：从长期来看，当进行大量重复的随机试验时，随机变量的实际取值会围绕其期望波动，并且随着试验次数的增加，这种波动会逐渐稳定在期望附近。例如，多次掷骰子后，统计每次掷出的点数并计算平均值，随着掷骰子次数的增多，这个平均值会越来越接近$\frac{1}{6} \times 1+\frac{1}{6} \times 2+\frac{1}{6} \times 1+\frac{1}{6} \times 3+\frac{1}{6} \times 4+\frac{1}{6} \times 5+\frac{1}{6} \times 6=3.5$。这就是大数定律的体现，它表明期望能够刻画随机变量在长期重复试验中的平均行为，使得随机性在大量数据的平均过程中被“平滑”或者说“消除”掉了。 
所以通过计算期望，能得到一个相对稳定、能代表随机变量总体特征的数值，减少了单次随机事件带来的不确定性和随机性影响，帮助我们从宏观上把握随机现象的本质特征。

![[Pasted image 20250301105032.png]]

如图所示，当前我们属于state t,假设我们已经观测到状态 $s_{t}$,而且做完决策,选中动作 $a_{t}$。那么$U_{t}$ 中的随机性来自于 t + 1 时刻起的所有的状态和动作,就是上图中黄色标出来的**未知变量部分**。
这里就是说，绿色部分是Agent已经有过的actions，通过这些actions与environment互动后，得到了实实在在的reward，而不是通过求概率得到的。
而后面黄色部分表出来的的**未知变量**部分，Agent还没有走到，但是我们要知道后面的State和Action的概率分布。

对 $U_{t}$关于变量$A_{t},R_{t},S{t},A_{t+1},R_{t=1},\dots,S_{n},A_{n},R_{n}.$求条件期望,得到$$Q_{\pi}(s_{t},a_{t})=\mathbb {E}_{S_{t+1},A_{t+1},\dots,S_{n},A_{n}}[U_{t} \mid S_{t}=s_{t},A_{t}=a{t}]$$
$S_{t}=s_{t},A_{t}=a{t}$是条件，就是说已经观察到了$S_{t}和A_{t}$的值了。条件期望的结果$Q_{\pi}(s_{t},a_{t})$被称为**动作价值函数(action-value function)**。
- 动作价值函数$Q_{\pi}(s_{t},a_{t})$**依赖于$S_{t}$与 $A_{t}$,** 而不依赖于$t+1$时刻及其之后的状态和动作,因为随机变量 $S_{t+1}, A_{t+1},\dots, S_{n}, A_{n}$ 都被期望消除了。
- 后续的Action都依赖于策略函数$\pi$，$\pi$不相同，则求取的期望也不同
#### 评价action value function依赖于三个因素
- **当前状态$S_{t}$** 当前状态就是上图中的绿色部分，如果当前状态越好，那么其价值$Q_{\pi}(s_{t},a_{t})$就越大。
- **当前动作$a_{t}$**  Agent执行的动作越好，则$Q_{\pi}(s_{t},a_{t})$就越大。
- **策略函数$\pi$** 策略决定了上图中黄色的未知部分的分布函数，策略函数越好，则 $Q_{\pi}(s_{t},a_{t})$就越大。

## 2.2 最优动作价值函数 optimal action-value function
由上述的的action value情况来看，还是因为策略的不确定性导致值的不确定性。为了抵消策略$\pi$的影响，就是找到一个最优动作价值函数 optimal action-value function$$Q_{\star}(s_{t},a_{t})=\max_{\pi} Q_{\pi}(s_{t},a_{t}),\forall s_{t} \in \mathcal{S}, a_{t} \in \mathcal{A} $$
就是在所有的策略中找到最好的策略函数$$\pi^{\star} = arg\max_{\pi} Q_{\pi}(s_{t},a_{t}) s_{t} \in \mathcal{S}, a_{t} \in \mathcal{A} $$
最优价值函数就像一个 **“先知”** ,指引Agent在后面的每一步都做出正确的选择，也就是在每一步获得最大的total reward。

## 2.3 状态价值函数 State Valuefunction
$$\begin{equation}
\begin{aligned}V_{\pi}(S_{t})&=\mathbb{E}_{A_{t\sim\pi(\cdot \mid s_{t})}}[Q_{\pi}(s_{t},A_{t})] \\
&=\sum_{a\in\mathcal{A}}\pi(a \mid s_{t})\cdot Q_{\pi}(s_{t},a).
\end{aligned}
\end{equation}
$$
$V_{\pi}(s_t)$的表达式，用于衡量在策略$\pi$下，处于状态$s_t$时智能体Agent未来能获得的期望回报，以下是具体解释： 
- **第一行**：$\mathbb{E}$表示期望Eexpectation，$A_t\sim\pi(\cdot|s_t)$说明期望是基于策略$\pi$在状态$s_{t}$下采取的动作$A_{t}$来计算；$Q_{\pi}(s_{t}, A_{t})$是在策略$\pi$下，状态$s_{t}$采取动作$A_{t}$后的动作价值函数，即从状态$s_{t}$执行动作$A_{t}$后，遵循策略$\pi$能获得的期望累计奖励。整行表示$V_{\pi}(s_t)$是在策略$\pi$下，对状态$s_{t}$采取不同动作后的动作价值函数$Q_{\pi}(s_{t}, A_{t})$取期望。
- **第二行**：是第一行的期望表达式展开。把动作$A_{t}$作为随机变量，然后求它的期望值，把它消除掉。表示对动作空间$\mathcal{A}$中所有可能的动作$a$进行求和；$\pi(a \mid s_{t})$是在策略$\pi$下，状态$s_{t}$采取动作$a$的概率 整行通过概率加权求和的方式
- 直白的说，就是把采取各个可能动作$a$（$a$属于动作空间$\mathcal{A}$）的概率$\pi(a|s_t)$ ，与对应动作价值$Q_{\pi}(s_t, a)$相乘，再将所有乘积求和，就能得到这个状态$s_t$在策略$\pi$下的价值$V_{\pi}(s_t)$ 。
最后得到的$V_{\pi}(S_{t})$值依赖于策略$\pi$与当前状态$s_{t}$，不依赖于动作a了。
$$V_{\pi}(s_t) = \mathbb{E}_{A_t,S_{t + 1},A_{t + 1},\cdots,S_{n},A_{n}}\left[U_t \middle| S_t = s_t\right].$$
可以看出，这个公式消除掉了$\mathbb{E}_{A_{t\sim\pi(\cdot \mid s_{t})}}[Q_{\pi}(s_{t},A_{t})]$中的$A_{t}$，这就意味$V_{\pi}(s_t)$越大，回报的期望就越大。
