## On Policy(同策略) 与 Off Policy（异策略）

### 行为策略
用于**采集经验（experience）**、即观测到的状态、动作、奖励。
是**用于生成数据**的策略。
### 目标策略
**强化学习的目的**是通过学习获取一个能是Agent获取最大收益的策略函数，该策略函数就叫做**目标策略**
是**用于控制Agent**的策略。

### On Policy(同策略)
使用相同的行为策略和目标策略

### Off Policy（异策略）
使用不同的行为策略和目标策略


### 两者的关系

它们的**主要区别**在于**如何利用收集到的样本数据来进行策略学习和优化**，以下是具体解释：
#### On  Policy
- **定义**：On  Policy方法是指智能体在学习过程中，**直接使用当前正在学习的策略来生成用于学习的数据**。也就是说，学习过程中使用的样本数据是由当前正在优化的策略所产生的。![[Pasted image 20250304145743.png]]

- **原理**：它基于这样的思想，即通过让智能体按照**当前**策略与环境进行交互，收集到的经验样本直接反映了**当前**策略的性能和效果。然后，根据这些样本数据来更新策略，使得策略在后续的交互中能够取得更好的性能。这种方式使得学习过程紧密地与**当前**策略相关联，每一步的学习都直接基于当前策略所产生的经验。 
- **算法示例**：经典的On - policy算法有策略梯度算法（如A2C、A3C）等。以A2C（Advantage Actor - Critic）算法为例，智能体根据当前的策略网络（Actor）生成动作与环境进行交互，得到奖励和新的状态等信息，然后利用这些信息来更新策略网络和价值网络（Critic）。在这个过程中，用于学习的数据完全是由当前正在训练的策略生成的。
#### Off Policy
- **定义**：Off Policy方法则是使用一个与当前正在学习的策略不同的策略来生成用于学习的数据。 ![[Pasted image 20250304145754.png]]
- **原理**：Off Policy方法的核心思想是利用**已有**的数据来学习一个最优策略，而这些数据**不一定是由当前正在学习的最优策略产生的**。通过巧妙地设计算法，可以**从这些“离线”的数据中提取出有用的信息**，来优化目标策略。这样做的**好处是可以利用大量的历史数据进行学习**，而不必局限于当前策略所产生的数据，从而提高学习的效率和稳定性。 
- **算法示例**：常见的Off Policy算法有Q - learning、深度Q网络（DQN）及其扩展，还有DDPG（Deep Deterministic Policy Gradient）等。以DQN为例，它使用一个经验回放缓冲区（experience replay buffer）来存储智能体与环境交互过程中产生的样本数据。在学习时，从缓冲区中随机采样数据来更新Q网络，而这些数据可能是由不同的探索策略产生的，与当前正在优化的Q网络所对应的策略并不一定相同。 
总的来说，On Policy方法直接利用当前策略生成的数据进行学习，能够紧密跟踪当前策略的性能并及时调整，但可能受到数据量和探索效率的限制；Off Policy方法则通过使用不同的策略生成数据，能够更灵活地利用大量历史数据进行学习，提高学习的稳定性和效率，但需要更复杂的算法来处理数据和估计目标策略的价值。