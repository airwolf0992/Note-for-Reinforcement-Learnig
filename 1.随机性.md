
# 随机性的来源

随机性的来源是**策略函数**与**状态转移函数**

**动作的随机性**来自于**随机决策**。
给定当前状态s策略函数  $\pi(a\mid s)$  会算出动作 空间$\mathcal{A}$  中每个动作a 的概率值。智能体执行的动作是随机抽样的结果，所以带有随机性。
![[Pasted image 20250301100446.png]]
**状态的随机性**来自于**状态转移函数**。  
当状态s和动作a都被确定下来,下一个状态仍然有随机性。
![[Pasted image 20250301100812.png]]

**奖励**是**状态**和**动作**的函数。
假设t 时刻的奖励是 $(s_{t},a_{t})$的函数,记作:  $$r_{t} = r(s_{t},a_{t})$$基于这种假设,给定当前状态$s_{t}$和动作$a_{t}$,  那么奖励 r_{t}就是唯一确定的。
**如果$A_{t}$还没被观测到,或者$s_{t},a_{t}$ 都没被观测到,  那么t时刻的奖励就有不确定性。**
我们用$$  R_{t} = r(s_{t}, A_{t}) $$  或   $$R_{t} = r(S_{t}, A_{t}  )  $$表示 t 时刻的奖励随机变量,它的随机性来自于$A_{t}$或者 $(S_{t}, A_{t})$。

## 马尔可夫性质(Markov property)


$$ \mathbb{P}(S_{t+1} \mid S_{t},A_{t})=\mathbb{P}(S_{t+1} \mid S_{1},A_{1},S_{2},A_{2},\dots S_{t},A_{t})$$
下一时刻状态 St+1 **仅依赖于当前状态 St 和动作 At**,而**不依赖于过去**的状态和动作。

### 轨迹 （trajectory）（回合）

指一回合(episode)游戏中,智能体观测到的所有的状态、动作、奖励:  $$s_{1}, a_{1}, r_{1}, s_{2}, a_{2}, r_{2},s_{3}, a_{3}, r_{3},\dots$$![[Pasted image 20250301102002.png]]

### 回报（return）与 折扣回报（discounted return)

#### 回报 return
从当前时刻**开始**到**本回合结束**的**所有奖励**的**总和**,所以回报也叫做**累计奖励(cumulative future reward)**。
把 t 时刻的回报记作随机变量 $U_{t}$。$$U_{t}=R_{t}+R_{t+1}+R_{t+2}+R_{t+3}+\dots+R_{n}$$
#### 回报的意义
- 回报是未来获得的奖励总和
- **智能体的目标**就是让**回报尽量大**,越大越好
- **强化学习的目标**就是寻找一个策略,**使得回报的期望最大化**。这个策略称为**最优策略 (optimum policy)**。

#### 折扣回报 discounted return

$$U_{t}=R_{t}+\gamma R_{t+1}+\gamma^{2} R_{t+2}+\gamma^{3} R_{t+3}+\dots$$
式中$\gamma \in [0,1]$称为**折扣率**,**对待越久远的未来,给奖励打的折扣越大**。在无限期 MDP 中,**折扣因子和奖励函数有界性**一起能保证上面无穷求和级数的**收敛性**。

#### 回报中的随机性

假设一回合游戏一共有 n 步。当完成这一回合之后,我们观测到所有 n 个奖励:  $r_{1}, r_{2}, \dots , r_{n}$。此时这些奖励**不是随机变量**,**而是实际观测到的数值**。其实际discounted return是$$u_{t}=r_{t}+\gamma r_{t+1}+\gamma^{2} r_{t+2}+\gamma^{3} r_{t+3}+\dots+\gamma^{n-t} r_{t+n},\forall t=1,\dots,n.$$
也可以写成$$ u_{t}={\textstyle \sum_{i=t}^{n}\gamma^{i-t}r_{i}} $$
![[Pasted image 20250301105032.png]]
假设我们此时在第 t 时刻,只观测到 st 及其之前的状态、动作、奖励$$s_{1},a_{1},r_{1},s_{2},a_{2},r_{2},\dots,s_{t-1},a_{t-1},r_{t-1},s_{t}$$是**已知的**
而后续的$$A_{t},R_{t},S{t},A_{t+1},R_{t=1},\dots,S_{n},A_{n},R_{n}.$$都是**未知的**。回报$U_{t}$依赖于A,R，而这些量是**未知的**，所以$U_{t}$也是**随机变量**。

#### 有限期MDP和无限期MDP
- **有限期  MDP (finite-horizon)** 存在一个终止状态(terminal state),该状态被智能体触发后,一个回合(episode)  结束。
- **有限期  MDP (infinite-horizon)** 的环境中不存在终止状态，这会导致奖励的加和趋于无穷。在这种MDP中如果$\gamma=1$会导致回报等于无穷，所以设置$\gamma$小于1的折扣率能够保证$u_{t}$收敛。